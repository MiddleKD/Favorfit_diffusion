{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/media/mlfavorfit/sdb/contolnet_dataset/control_net_train/train.jsonl'\n",
    "\n",
    "text_dict = {}\n",
    "with jsonlines.open(file_path, 'r') as reader:\n",
    "    for obj in reader:\n",
    "        text_dict[obj[\"image\"]] = obj[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"images\"\n",
    "condition_dir = \"conditioning_images\"\n",
    "text_dict = text_dict\n",
    "\n",
    "def preprocess_line(obj):\n",
    "    dn = os.path.basename(os.path.dirname(obj['fn']))\n",
    "    fn = dn + \"_\" + os.path.basename(obj['fn']) + \".jpg\"\n",
    "\n",
    "    image_fn = os.path.join(image_dir, fn)\n",
    "    condition_fn = os.path.join(condition_dir, fn)\n",
    "    return {\"image\":image_fn, \n",
    "            \"conditioning_images\":condition_fn, \n",
    "            \"obj_colors\":obj[\"obj_colors\"], \n",
    "            \"bg_colors\":obj[\"bg_colors\"], \n",
    "            \"total_colors\":obj[\"total_colors\"],\n",
    "            \"text\":text_dict[image_fn]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mlfavorfit/lib/favorfit/kjg/color_and_template_recommendation/favorfit_product_color_datas.jsonl'\n",
    "\n",
    "data_list = []\n",
    "with jsonlines.open(file_path, 'r') as reader:\n",
    "    for obj in reader:\n",
    "        data_list.append(preprocess_line(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'output_jsonline_file.jsonl'  # 쓰기용 파일 경로를 적절하게 수정하세요.\n",
    "\n",
    "with jsonlines.open(file_path, 'w') as writer:\n",
    "    writer.write_all(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import UNET\n",
    "\n",
    "unet = UNET(is_lora=True)\n",
    "my_model = unet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_lora = my_model.copy()\n",
    "for cur in my_model.keys():\n",
    "    if \"lora\" not in cur:\n",
    "        my_model_lora.pop(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "pre_model = load_file(\"/home/mlfavorfit/lib/favorfit/kjg/0_model_weights/diffusion/lora/FavorfitStyle.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns = {\"encoders\":\"down_blocks\", \"bottleneck\":\"mid_block\", \"decoders\":\"up_blocks\"}\n",
    "qkv = [\"k\",\"q\",\"v\",\"out\"]\n",
    "ud = [\"up\",\"down\"]\n",
    "encoder_layer_map = {1:(0,0), 2:(0,1), 4:(1,0), 5:(1,1), 7:(2,0), 8:(2,1)}\n",
    "decoder_layer_map = {3:(1,0), 4:(1,1), 5:(1,2), \n",
    "                     6:(2,0), 7:(2,1), 8:(2,2),\n",
    "                     9:(3,0), 10:(3,1), 11:(3,2),}\n",
    "\n",
    "weight_map = []\n",
    "for ln in lns.keys():\n",
    "    if ln == \"encoders\":\n",
    "        for layer in encoder_layer_map.keys():\n",
    "            for attn in [1,2]:\n",
    "                for t in qkv:\n",
    "                    for d in ud:\n",
    "                        weight_map.append(\n",
    "                            (f\"{ln}.{layer}.1.attention_{attn}.{t}_lora_{d}.weight\",\n",
    "                                f\"{lns[ln]}.{encoder_layer_map[layer][0]}.attentions.{encoder_layer_map[layer][1]}.transformer_blocks.0.attn{attn}.processor.to_{t}_lora.{d}.weight\")\n",
    "                        )\n",
    "    elif ln == \"decoders\":\n",
    "        for layer in decoder_layer_map.keys():\n",
    "            for attn in [1,2]:\n",
    "                for t in qkv:\n",
    "                    for d in ud:\n",
    "                        weight_map.append(\n",
    "                            (f\"{ln}.{layer}.1.attention_{attn}.{t}_lora_{d}.weight\",\n",
    "                                f\"{lns[ln]}.{decoder_layer_map[layer][0]}.attentions.{decoder_layer_map[layer][1]}.transformer_blocks.0.attn{attn}.processor.to_{t}_lora.{d}.weight\")\n",
    "                        )\n",
    "    else:\n",
    "        for attn in [1,2]:\n",
    "            for t in qkv:\n",
    "                for d in ud:\n",
    "                    weight_map.append(\n",
    "                        (f\"{ln}.1.attention_{attn}.{t}_lora_{d}.weight\",\n",
    "                            f\"{lns[ln]}.attentions.0.transformer_blocks.0.attn{attn}.processor.to_{t}_lora.{d}.weight\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_weight = {}\n",
    "for my_key, pre_key in weight_map:\n",
    "    if my_model_lora[my_key].shape == pre_model[pre_key].shape:\n",
    "        new_model_weight[my_key] = pre_model[pre_key]\n",
    "    else:\n",
    "        print(my_model_lora[my_key].shape, pre_model[pre_key].shape)\n",
    "        print((my_key, pre_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_model_weight) == len(pre_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(new_model_weight, \"my_own_lora.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_tiles(image_path, tile_size):\n",
    "    # 이미지 열기\n",
    "    img = Image.open(image_path).resize([512,512])\n",
    "\n",
    "    # 이미지 크기 가져오기\n",
    "    width, height = img.size\n",
    "\n",
    "    # 8x8 타일 크기 설정\n",
    "    tile_width, tile_height = tile_size\n",
    "\n",
    "    # 타일별로 이미지를 자르고 각 타일의 평균 색상 계산\n",
    "    for y in range(0, height, tile_height):\n",
    "        for x in range(0, width, tile_width):\n",
    "            # 타일 자르기\n",
    "            tile = img.crop((x, y, x + tile_width, y + tile_height))\n",
    "\n",
    "            # 타일의 평균 색상 계산\n",
    "            average_color = np.array(tile).mean(axis=(0, 1)).astype(int)\n",
    "\n",
    "            # 타일에 평균 색상 적용\n",
    "            tile = Image.fromarray(np.full((tile_height, tile_width, 3), average_color, dtype=np.uint8))\n",
    "\n",
    "            # 원본 이미지에 타일 적용\n",
    "            img.paste(tile, (x, y, x + tile_width, y + tile_height))\n",
    "\n",
    "    # 결과 이미지 저장 또는 표시\n",
    "    # img.show()\n",
    "    return img\n",
    "\n",
    "# 이미지 경로와 타일 크기 설정\n",
    "image_path = './images/bottle.jpg'  # 이미지 파일 경로로 바꾸세요\n",
    "tile_size = (512//4, 512//4)\n",
    "\n",
    "# 함수 호출\n",
    "convert_to_tiles(image_path, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "tile_size = (512//8, 512//8)\n",
    "\n",
    "fns = glob(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_base/images/*\")\n",
    "save_dir = \"/media/mlfavorfit/sdb/contolnet_dataset/temp\"\n",
    "for fn in tqdm(fns, total=len(fns)):\n",
    "    img = convert_to_tiles(fn, tile_size)\n",
    "    img.save(os.path.join(save_dir, os.path.basename(fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/media/mlfavorfit/sdb/contolnet_dataset/temp3\"\n",
    "\n",
    "def combine_images(img_fn, mask_fn, color_fn):\n",
    "    fn = os.path.basename(img_fn)\n",
    "    img = np.array(Image.open(img_fn).convert(\"RGB\"))\n",
    "    mask = np.array(Image.open(mask_fn).convert(\"RGB\"))\n",
    "    color = np.array(Image.open(color_fn).convert(\"RGB\"))\n",
    "\n",
    "    result_np = img * (mask/255.0) + color * (1-mask/255.0)\n",
    "    result_image = Image.fromarray(result_np.astype(np.uint8))\n",
    "    \n",
    "    result_image.save(os.path.join(save_dir, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/197967 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "image_fns = glob(\"/media/mlfavorfit/sdb/product_512_dataset/images/*.jpg\")\n",
    "mask_fns = glob(\"/media/mlfavorfit/sdb/product_512_dataset/masks/*.jpg\")\n",
    "color_fns = glob(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_base/conditioning_images_8x8/*.jpg\")\n",
    "\n",
    "for img_fn, mask_fn, color_fn in tqdm(zip(image_fns, mask_fns, color_fns), total=len(image_fns)):\n",
    "    img = combine_images(img_fn, mask_fn, color_fn)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "controlent_dict = torch.load(\"./checkpoint-115000/controlnet_1.pth\")\n",
    "embedding_dict = torch.load(\"./checkpoint-115000/embedding_1.pth\")\n",
    "total_dict = {\"controlnet\":controlent_dict, \"embedding\": embedding_dict}\n",
    "\n",
    "torch.save(total_dict, \"./controlnet_colot_tile.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def convert_to_random_color_tiles(colors, image_size=[512,512], tile_num_per_row=4):\n",
    "    # 이미지 열기\n",
    "    img = Image.new('RGB', image_size)\n",
    "\n",
    "    # 이미지 크기 가져오기\n",
    "    width, height = img.size\n",
    "    tile_size = (512//tile_num_per_row, 512//tile_num_per_row)\n",
    "\n",
    "    # 타일별로 이미지를 자르고 랜덤한 색상의 타일로 대체\n",
    "    for y in range(0, height, tile_size[1]):\n",
    "        for x in range(0, width, tile_size[0]):\n",
    "            # 랜덤하게 색상 섞기\n",
    "            random.shuffle(colors)\n",
    "\n",
    "            # 랜덤한 타일 생성\n",
    "            tile = Image.new('RGB', tile_size, tuple(colors[0]))\n",
    "\n",
    "            # 원본 이미지에 랜덤한 타일 적용\n",
    "            img.paste(tile, (x, y, x + tile_size[0], y + tile_size[1]))\n",
    "\n",
    "    # 결과 이미지 저장 또는 표시\n",
    "    # img.show()\n",
    "    return img\n",
    "\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # 빨간색\n",
    "    (0, 255, 0),  # 초록색\n",
    "    (0, 0, 255),  # 파란색\n",
    "    (255, 255, 0),  # 노란색\n",
    "]\n",
    "\n",
    "# 함수 호출\n",
    "convert_to_random_color_tiles(colors, image_size=[512,512], tile_num_per_row=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.C_layer1 = nn.Linear(8,4)\n",
    "        self.C_layer2 = nn.Linear(4,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.C_layer1(x)\n",
    "        x = self.C_layer2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class AModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2,4)\n",
    "        self.layer2 = nn.Linear(4,8)\n",
    "        self.C = CModel()\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.C(x)\n",
    "        return x\n",
    "    def get_C(self):\n",
    "        return self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = AModel()\n",
    "a_state_dict = model_a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, state_dict) -> None:\n",
    "        super().__init__()\n",
    "        for key, value in state_dict.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = {}\n",
    "\n",
    "def recursice_named_children(name, cur_module):\n",
    "    for subname, module in cur_module.named_children():\n",
    "        recursice_named_children(f\"{name}.{subname}\",module)\n",
    "        if \"C\" in subname:\n",
    "            processor[f\"{name}.{subname}\"] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_modules = model_a.named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C',\n",
       " CModel(\n",
       "   (C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "   (C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_model = iter(cur_modules).__next__()\n",
    "cur_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursice_named_children(cur_model[0], cur_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C.C_layer1': Linear(in_features=8, out_features=4, bias=True),\n",
       " 'C.C_layer2': Linear(in_features=4, out_features=2, bias=True)}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = WrappedModel(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  (C.C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (C.C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            x = torch.rand(1)  # x는 랜덤 값\n",
    "            y = torch.rand(1)  # y는 랜덤 값\n",
    "            a = 3 * x + 2 * y\n",
    "            b = torch.rand(1)  # b는 랜덤 값\n",
    "\n",
    "            samples.append({'input': torch.cat([x, y]), 'output': torch.cat([a, b])})\n",
    "        self.samples = samples\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        temp_dict = self.samples[idx]\n",
    "        return temp_dict[\"input\"], temp_dict[\"output\"]\n",
    "\n",
    "# 데이터셋 생성\n",
    "num_samples = 10\n",
    "simple_dataset = SimpleDataset(num_samples)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(simple_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "            wrapped_model.parameters(),\n",
    "            lr=0.1,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-2,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lr_scheduler = LambdaLR(optimizer, lambda _: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        wrapped_model, optimizer, train_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AModel(\n",
       "  (layer1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (layer2): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (C): CModel(\n",
       "    (C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, 0.004987453576177359\n",
      "Epoch 1, 0.010965308174490929\n",
      "Epoch 2, 0.003895475761964917\n",
      "Epoch 3, 0.022672384977340698\n",
      "Epoch 4, 0.04985092952847481\n",
      "Epoch 5, 0.017257962375879288\n",
      "Epoch 6, 0.0031657549552619457\n",
      "Epoch 7, 0.008375853300094604\n",
      "Epoch 8, 0.03736884891986847\n",
      "Epoch 9, 0.010994495823979378\n",
      "Epoch 10, 0.0665440559387207\n",
      "Epoch 11, 0.0022774708922952414\n",
      "Epoch 12, 0.005960617680102587\n",
      "Epoch 13, 0.0010876852320507169\n",
      "Epoch 14, 0.06709764897823334\n",
      "Epoch 15, 0.013005584478378296\n",
      "Epoch 16, 0.012147202156484127\n",
      "Epoch 17, 0.006254617124795914\n",
      "Epoch 18, 0.03644535690546036\n",
      "Epoch 19, 0.02989388257265091\n",
      "Epoch 20, 0.009469026699662209\n",
      "Epoch 21, 0.00793942529708147\n",
      "Epoch 22, 0.007803367450833321\n",
      "Epoch 23, 0.005954444408416748\n",
      "Epoch 24, 0.021796928718686104\n",
      "Epoch 25, 0.010215112008154392\n",
      "Epoch 26, 0.013807850889861584\n",
      "Epoch 27, 0.010160761885344982\n",
      "Epoch 28, 0.003038141643628478\n",
      "Epoch 29, 0.0077806562185287476\n",
      "Epoch 30, 0.007165942341089249\n",
      "Epoch 31, 0.061013124883174896\n",
      "Epoch 32, 0.0341915637254715\n",
      "Epoch 33, 0.013343066908419132\n",
      "Epoch 34, 0.007224639877676964\n",
      "Epoch 35, 0.005325806327164173\n",
      "Epoch 36, 0.010024191811680794\n",
      "Epoch 37, 0.015134020708501339\n",
      "Epoch 38, 0.0019995172042399645\n",
      "Epoch 39, 0.012619203887879848\n",
      "Epoch 40, 0.014865190722048283\n",
      "Epoch 41, 0.009966625832021236\n",
      "Epoch 42, 0.04331228509545326\n",
      "Epoch 43, 0.05455942079424858\n",
      "Epoch 44, 0.04373200237751007\n",
      "Epoch 45, 0.030984193086624146\n",
      "Epoch 46, 0.01423456147313118\n",
      "Epoch 47, 0.03554736450314522\n",
      "Epoch 48, 0.05463331937789917\n",
      "Epoch 49, 0.00956244207918644\n",
      "Epoch 50, 0.0039027389138936996\n",
      "Epoch 51, 0.01062126737087965\n",
      "Epoch 52, 0.014757794328033924\n",
      "Epoch 53, 0.017134856432676315\n",
      "Epoch 54, 0.009159999899566174\n",
      "Epoch 55, 0.058278393000364304\n",
      "Epoch 56, 0.0029855379834771156\n",
      "Epoch 57, 0.02067405730485916\n",
      "Epoch 58, 0.012882783077657223\n",
      "Epoch 59, 0.008692401461303234\n",
      "Epoch 60, 0.00411298219114542\n",
      "Epoch 61, 0.043036818504333496\n",
      "Epoch 62, 0.012148987501859665\n",
      "Epoch 63, 0.0128182303160429\n",
      "Epoch 64, 0.030758734792470932\n",
      "Epoch 65, 0.02090325392782688\n",
      "Epoch 66, 0.019145797938108444\n",
      "Epoch 67, 0.008910870179533958\n",
      "Epoch 68, 0.04648100584745407\n",
      "Epoch 69, 0.010454675182700157\n",
      "Epoch 70, 0.00391951622441411\n",
      "Epoch 71, 0.008387398906052113\n",
      "Epoch 72, 0.01748676225543022\n",
      "Epoch 73, 0.006397963967174292\n",
      "Epoch 74, 0.003748843213543296\n",
      "Epoch 75, 0.0053724078461527824\n",
      "Epoch 76, 0.008206558413803577\n",
      "Epoch 77, 0.03745366632938385\n",
      "Epoch 78, 0.04186892509460449\n",
      "Epoch 79, 0.03285672888159752\n",
      "Epoch 80, 0.0549330934882164\n",
      "Epoch 81, 0.028501585125923157\n",
      "Epoch 82, 0.010355866514146328\n",
      "Epoch 83, 0.04426479712128639\n",
      "Epoch 84, 0.01386206317692995\n",
      "Epoch 85, 0.00922293309122324\n",
      "Epoch 86, 0.009732170030474663\n",
      "Epoch 87, 0.06178929656744003\n",
      "Epoch 88, 0.03374107927083969\n",
      "Epoch 89, 0.018833084031939507\n",
      "Epoch 90, 0.07613281905651093\n",
      "Epoch 91, 0.004353155381977558\n",
      "Epoch 92, 0.016735944896936417\n",
      "Epoch 93, 0.023690083995461464\n",
      "Epoch 94, 0.06324773281812668\n",
      "Epoch 95, 0.0098255705088377\n",
      "Epoch 96, 0.014916911721229553\n",
      "Epoch 97, 0.01583162322640419\n",
      "Epoch 98, 0.04764001816511154\n",
      "Epoch 99, 0.009263422340154648\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        out = model_a(x)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(out, y, reduction=\"mean\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=False)\n",
    "\n",
    "    print(f\"Epoch {epoch}, {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.2637, -0.2863],\n",
       "                      [ 0.4577,  0.5389],\n",
       "                      [ 0.2914,  0.1599],\n",
       "                      [-0.5159,  0.3440]], device='cuda:0')),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.4801, -0.2358,  0.0052,  0.0658], device='cuda:0')),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.4251,  0.2755,  0.2153, -0.4284],\n",
       "                      [ 0.2034, -0.4190, -0.4411, -0.1232],\n",
       "                      [ 0.0289,  0.2993,  0.2970,  0.1596],\n",
       "                      [ 0.2103, -0.3195,  0.2618,  0.0103],\n",
       "                      [-0.3904, -0.0397, -0.4616,  0.1199],\n",
       "                      [ 0.1592,  0.3799, -0.3946, -0.1098],\n",
       "                      [ 0.2867, -0.2817,  0.2606,  0.3815],\n",
       "                      [-0.0480,  0.0790, -0.3579, -0.0761]], device='cuda:0')),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.4700,  0.0951,  0.1737,  0.4081, -0.3703, -0.4907, -0.2209, -0.2582],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.weight',\n",
       "              tensor([[-0.6404,  1.6123, -0.2945,  0.2490,  0.1103, -0.2565,  1.9435,  0.0822],\n",
       "                      [-0.7081,  1.7357, -0.3706,  0.4704,  0.2471, -0.2997,  1.7364,  0.0578],\n",
       "                      [ 0.0851, -0.6334,  0.0023, -0.1194, -0.0858, -0.1380, -0.2135, -0.0104],\n",
       "                      [ 0.2139, -1.5487,  0.6958, -0.4044, -0.2944,  0.1729, -0.6144, -0.0769]],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.bias',\n",
       "              tensor([ 0.0594,  0.0100,  0.0262, -0.0822], device='cuda:0')),\n",
       "             ('C.C_layer2.weight',\n",
       "              tensor([[-0.7848, -0.9182,  0.1813,  0.4802],\n",
       "                      [-0.1006, -0.0812, -0.0134, -0.1198]], device='cuda:0')),\n",
       "             ('C.C_layer2.bias', tensor([0.8199, 0.4172], device='cuda:0'))])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.2637, -0.2863],\n",
       "                      [ 0.4577,  0.5389],\n",
       "                      [ 0.2914,  0.1599],\n",
       "                      [-0.5159,  0.3440]], device='cuda:0')),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.4801, -0.2358,  0.0052,  0.0658], device='cuda:0')),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.4251,  0.2755,  0.2153, -0.4284],\n",
       "                      [ 0.2034, -0.4190, -0.4411, -0.1232],\n",
       "                      [ 0.0289,  0.2993,  0.2970,  0.1596],\n",
       "                      [ 0.2103, -0.3195,  0.2618,  0.0103],\n",
       "                      [-0.3904, -0.0397, -0.4616,  0.1199],\n",
       "                      [ 0.1592,  0.3799, -0.3946, -0.1098],\n",
       "                      [ 0.2867, -0.2817,  0.2606,  0.3815],\n",
       "                      [-0.0480,  0.0790, -0.3579, -0.0761]], device='cuda:0')),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.4700,  0.0951,  0.1737,  0.4081, -0.3703, -0.4907, -0.2209, -0.2582],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.weight',\n",
       "              tensor([[-0.5453,  1.2696, -0.1563,  0.2514,  0.1227, -0.2533,  1.6813,  0.0064],\n",
       "                      [-0.5816,  1.3821, -0.2795,  0.3671,  0.2207, -0.2402,  1.4870,  0.0354],\n",
       "                      [ 0.0124, -0.6204,  0.0672, -0.1521, -0.0164, -0.0456, -0.1282,  0.0055],\n",
       "                      [-0.0061, -1.6783,  0.7161, -0.4285, -0.0914,  0.1432, -0.2589, -0.0799]],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.bias',\n",
       "              tensor([ 0.0851,  0.0155, -0.0060, -0.0499], device='cuda:0')),\n",
       "             ('C.C_layer2.weight',\n",
       "              tensor([[-0.9567, -1.1084,  0.1827,  0.5333],\n",
       "                      [-0.1143, -0.1241,  0.0079, -0.0094]], device='cuda:0')),\n",
       "             ('C.C_layer2.bias', tensor([0.9837, 0.4588], device='cuda:0'))])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): CModel(\n",
       "      (layer1): Linear(in_features=4, out_features=3, bias=True)\n",
       "      (layer2): Linear(in_features=3, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.unwrap_model(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layers.0.layer1.weight',\n",
       "              tensor([[-0.9413,  1.8445,  0.1948,  0.1523],\n",
       "                      [ 1.8247, -4.1675,  0.2298,  0.1094],\n",
       "                      [-1.1304,  0.8007,  0.3672, -0.0239]], device='cuda:0')),\n",
       "             ('layers.0.layer1.bias',\n",
       "              tensor([-0.2499,  0.4548, -0.5478], device='cuda:0')),\n",
       "             ('layers.0.layer2.weight',\n",
       "              tensor([[ 0.8566, -2.6253,  0.3337],\n",
       "                      [-0.2639,  0.2939, -0.2823]], device='cuda:0')),\n",
       "             ('layers.0.layer2.bias',\n",
       "              tensor([0.2533, 0.8179], device='cuda:0'))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader import load_diffusion_model\n",
    "\n",
    "kwargs = {\"is_lora\":True, \"lora_scale\":1.0}\n",
    "models = load_diffusion_model(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = models['diffusion'].named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_module = iter(module_list).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = {}\n",
    "\n",
    "def recursice_named_children(name, cur_module):\n",
    "    for subname, module in cur_module.named_children():\n",
    "        recursice_named_children(f\"{name}.{subname}\",module)\n",
    "        if \"lora\" in subname:\n",
    "            processor[f\"{name}.{subname}\"] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursice_named_children(cur_module[0], cur_module[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unet.encoders.1.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False)}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
