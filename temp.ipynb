{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/media/mlfavorfit/sdb/contolnet_dataset/control_net_train/train.jsonl'\n",
    "\n",
    "text_dict = {}\n",
    "with jsonlines.open(file_path, 'r') as reader:\n",
    "    for obj in reader:\n",
    "        text_dict[obj[\"image\"]] = obj[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"images\"\n",
    "condition_dir = \"conditioning_images\"\n",
    "text_dict = text_dict\n",
    "\n",
    "def preprocess_line(obj):\n",
    "    dn = os.path.basename(os.path.dirname(obj['fn']))\n",
    "    fn = dn + \"_\" + os.path.basename(obj['fn']) + \".jpg\"\n",
    "\n",
    "    image_fn = os.path.join(image_dir, fn)\n",
    "    condition_fn = os.path.join(condition_dir, fn)\n",
    "    return {\"image\":image_fn, \n",
    "            \"conditioning_images\":condition_fn, \n",
    "            \"obj_colors\":obj[\"obj_colors\"], \n",
    "            \"bg_colors\":obj[\"bg_colors\"], \n",
    "            \"total_colors\":obj[\"total_colors\"],\n",
    "            \"text\":text_dict[image_fn]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mlfavorfit/lib/favorfit/kjg/color_and_template_recommendation/favorfit_product_color_datas.jsonl'\n",
    "\n",
    "data_list = []\n",
    "with jsonlines.open(file_path, 'r') as reader:\n",
    "    for obj in reader:\n",
    "        data_list.append(preprocess_line(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'output_jsonline_file.jsonl'  # 쓰기용 파일 경로를 적절하게 수정하세요.\n",
    "\n",
    "with jsonlines.open(file_path, 'w') as writer:\n",
    "    writer.write_all(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import UNET\n",
    "\n",
    "unet = UNET(is_lora=True)\n",
    "my_model = unet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_lora = my_model.copy()\n",
    "for cur in my_model.keys():\n",
    "    if \"lora\" not in cur:\n",
    "        my_model_lora.pop(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "pre_model = load_file(\"/home/mlfavorfit/lib/favorfit/kjg/0_model_weights/diffusion/lora/FavorfitStyle.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns = {\"encoders\":\"down_blocks\", \"bottleneck\":\"mid_block\", \"decoders\":\"up_blocks\"}\n",
    "qkv = [\"k\",\"q\",\"v\",\"out\"]\n",
    "ud = [\"up\",\"down\"]\n",
    "encoder_layer_map = {1:(0,0), 2:(0,1), 4:(1,0), 5:(1,1), 7:(2,0), 8:(2,1)}\n",
    "decoder_layer_map = {3:(1,0), 4:(1,1), 5:(1,2), \n",
    "                     6:(2,0), 7:(2,1), 8:(2,2),\n",
    "                     9:(3,0), 10:(3,1), 11:(3,2),}\n",
    "\n",
    "weight_map = []\n",
    "for ln in lns.keys():\n",
    "    if ln == \"encoders\":\n",
    "        for layer in encoder_layer_map.keys():\n",
    "            for attn in [1,2]:\n",
    "                for t in qkv:\n",
    "                    for d in ud:\n",
    "                        weight_map.append(\n",
    "                            (f\"{ln}.{layer}.1.attention_{attn}.{t}_lora_{d}.weight\",\n",
    "                                f\"{lns[ln]}.{encoder_layer_map[layer][0]}.attentions.{encoder_layer_map[layer][1]}.transformer_blocks.0.attn{attn}.processor.to_{t}_lora.{d}.weight\")\n",
    "                        )\n",
    "    elif ln == \"decoders\":\n",
    "        for layer in decoder_layer_map.keys():\n",
    "            for attn in [1,2]:\n",
    "                for t in qkv:\n",
    "                    for d in ud:\n",
    "                        weight_map.append(\n",
    "                            (f\"{ln}.{layer}.1.attention_{attn}.{t}_lora_{d}.weight\",\n",
    "                                f\"{lns[ln]}.{decoder_layer_map[layer][0]}.attentions.{decoder_layer_map[layer][1]}.transformer_blocks.0.attn{attn}.processor.to_{t}_lora.{d}.weight\")\n",
    "                        )\n",
    "    else:\n",
    "        for attn in [1,2]:\n",
    "            for t in qkv:\n",
    "                for d in ud:\n",
    "                    weight_map.append(\n",
    "                        (f\"{ln}.1.attention_{attn}.{t}_lora_{d}.weight\",\n",
    "                            f\"{lns[ln]}.attentions.0.transformer_blocks.0.attn{attn}.processor.to_{t}_lora.{d}.weight\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_weight = {}\n",
    "for my_key, pre_key in weight_map:\n",
    "    if my_model_lora[my_key].shape == pre_model[pre_key].shape:\n",
    "        new_model_weight[my_key] = pre_model[pre_key]\n",
    "    else:\n",
    "        print(my_model_lora[my_key].shape, pre_model[pre_key].shape)\n",
    "        print((my_key, pre_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_model_weight) == len(pre_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(new_model_weight, \"my_own_lora.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_tiles(image_path, tile_size):\n",
    "    # 이미지 열기\n",
    "    img = Image.open(image_path).resize([512,512])\n",
    "\n",
    "    # 이미지 크기 가져오기\n",
    "    width, height = img.size\n",
    "\n",
    "    # 8x8 타일 크기 설정\n",
    "    tile_width, tile_height = tile_size\n",
    "\n",
    "    # 타일별로 이미지를 자르고 각 타일의 평균 색상 계산\n",
    "    for y in range(0, height, tile_height):\n",
    "        for x in range(0, width, tile_width):\n",
    "            # 타일 자르기\n",
    "            tile = img.crop((x, y, x + tile_width, y + tile_height))\n",
    "\n",
    "            # 타일의 평균 색상 계산\n",
    "            average_color = np.array(tile).mean(axis=(0, 1)).astype(int)\n",
    "\n",
    "            # 타일에 평균 색상 적용\n",
    "            tile = Image.fromarray(np.full((tile_height, tile_width, 3), average_color, dtype=np.uint8))\n",
    "\n",
    "            # 원본 이미지에 타일 적용\n",
    "            img.paste(tile, (x, y, x + tile_width, y + tile_height))\n",
    "\n",
    "    # 결과 이미지 저장 또는 표시\n",
    "    # img.show()\n",
    "    return img\n",
    "\n",
    "# 이미지 경로와 타일 크기 설정\n",
    "image_path = './images/bottle.jpg'  # 이미지 파일 경로로 바꾸세요\n",
    "tile_size = (512//4, 512//4)\n",
    "\n",
    "# 함수 호출\n",
    "convert_to_tiles(image_path, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "tile_size = (512//8, 512//8)\n",
    "\n",
    "fns = glob(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_base/images/*\")\n",
    "save_dir = \"/media/mlfavorfit/sdb/contolnet_dataset/temp\"\n",
    "for fn in tqdm(fns, total=len(fns)):\n",
    "    img = convert_to_tiles(fn, tile_size)\n",
    "    img.save(os.path.join(save_dir, os.path.basename(fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/media/mlfavorfit/sdb/contolnet_dataset/temp3\"\n",
    "\n",
    "def combine_images(img_fn, mask_fn, color_fn):\n",
    "    fn = os.path.basename(img_fn)\n",
    "    img = np.array(Image.open(img_fn).convert(\"RGB\"))\n",
    "    mask = np.array(Image.open(mask_fn).convert(\"RGB\"))\n",
    "    color = np.array(Image.open(color_fn).convert(\"RGB\"))\n",
    "\n",
    "    result_np = img * (mask/255.0) + color * (1-mask/255.0)\n",
    "    result_image = Image.fromarray(result_np.astype(np.uint8))\n",
    "    \n",
    "    result_image.save(os.path.join(save_dir, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/197967 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "image_fns = glob(\"/media/mlfavorfit/sdb/product_512_dataset/images/*.jpg\")\n",
    "mask_fns = glob(\"/media/mlfavorfit/sdb/product_512_dataset/masks/*.jpg\")\n",
    "color_fns = glob(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_base/conditioning_images_8x8/*.jpg\")\n",
    "\n",
    "for img_fn, mask_fn, color_fn in tqdm(zip(image_fns, mask_fns, color_fns), total=len(image_fns)):\n",
    "    img = combine_images(img_fn, mask_fn, color_fn)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "controlent_dict = torch.load(\"./checkpoint-115000/controlnet_1.pth\")\n",
    "embedding_dict = torch.load(\"./checkpoint-115000/embedding_1.pth\")\n",
    "total_dict = {\"controlnet\":controlent_dict, \"embedding\": embedding_dict}\n",
    "\n",
    "torch.save(total_dict, \"./controlnet_colot_tile.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAIAAAB7GkOtAAAHQ0lEQVR4nO3XwQ3AMAzEsLj77+yO4YfICQ4NUiHz3j6o2p3rCW0+/6nvegAANwQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAomb3ekLcXA9om+cCnFoX4JIXAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAEDUvLfXG4CofXM9Ic0LACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBKAACiBAAgSgAAogQAIEoAAKIEACBq9npBnQMgbPyBTnkBAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAEQJAECUAABECQBAlAAARAkAQJQAAET9NQkP+bW7qikAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def convert_to_random_color_tiles(colors, image_size=[512,512], tile_num_per_row=4):\n",
    "    # 이미지 열기\n",
    "    img = Image.new('RGB', image_size)\n",
    "\n",
    "    # 이미지 크기 가져오기\n",
    "    width, height = img.size\n",
    "    tile_size = (512//tile_num_per_row, 512//tile_num_per_row)\n",
    "\n",
    "    # 타일별로 이미지를 자르고 랜덤한 색상의 타일로 대체\n",
    "    for y in range(0, height, tile_size[1]):\n",
    "        for x in range(0, width, tile_size[0]):\n",
    "            # 랜덤하게 색상 섞기\n",
    "            random.shuffle(colors)\n",
    "\n",
    "            # 랜덤한 타일 생성\n",
    "            tile = Image.new('RGB', tile_size, tuple(colors[0]))\n",
    "\n",
    "            # 원본 이미지에 랜덤한 타일 적용\n",
    "            img.paste(tile, (x, y, x + tile_size[0], y + tile_size[1]))\n",
    "\n",
    "    # 결과 이미지 저장 또는 표시\n",
    "    # img.show()\n",
    "    return img\n",
    "\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # 빨간색\n",
    "    (0, 255, 0),  # 초록색\n",
    "    (0, 0, 255),  # 파란색\n",
    "    (255, 255, 0),  # 노란색\n",
    "]\n",
    "\n",
    "# 함수 호출\n",
    "convert_to_random_color_tiles(colors, image_size=[512,512], tile_num_per_row=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.C_layer1 = nn.Linear(8,4)\n",
    "        self.C_layer2 = nn.Linear(4,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.C_layer1(x)\n",
    "        x = self.C_layer2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class AModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2,4)\n",
    "        self.layer2 = nn.Linear(4,8)\n",
    "        self.C = CModel()\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.C(x)\n",
    "        return x\n",
    "    def get_C(self):\n",
    "        return self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = AModel()\n",
    "a_state_dict = model_a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AModel(\n",
       "  (layer1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (layer2): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (C): CModel(\n",
       "    (C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, state_dict) -> None:\n",
    "        super().__init__()\n",
    "        for key, value in state_dict.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = {}\n",
    "\n",
    "def recursice_named_children(name, cur_module):\n",
    "    for subname, module in cur_module.named_children():\n",
    "        recursice_named_children(f\"{name}.{subname}\",module)\n",
    "        if \"C\" in subname:\n",
    "            processor[f\"{name}.{subname}\"] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_modules = model_a.named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C',\n",
       " CModel(\n",
       "   (C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "   (C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_model = iter(cur_modules).__next__()\n",
    "cur_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursice_named_children(cur_model[0], cur_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C.C_layer1': Linear(in_features=8, out_features=4, bias=True),\n",
       " 'C.C_layer2': Linear(in_features=4, out_features=2, bias=True)}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  (C.C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (C.C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model = WrappedModel(processor)\n",
    "wrapped_model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  (C.C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (C.C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            x = torch.rand(1)  # x는 랜덤 값\n",
    "            y = torch.rand(1)  # y는 랜덤 값\n",
    "            a = 3 * x + 2 * y\n",
    "            b = torch.rand(1)  # b는 랜덤 값\n",
    "\n",
    "            samples.append({'input': torch.cat([x, y]), 'output': torch.cat([a, b])})\n",
    "        self.samples = samples\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        temp_dict = self.samples[idx]\n",
    "        return temp_dict[\"input\"], temp_dict[\"output\"]\n",
    "\n",
    "# 데이터셋 생성\n",
    "num_samples = 10\n",
    "simple_dataset = SimpleDataset(num_samples)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(simple_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "            wrapped_model.parameters(),\n",
    "            lr=0.1,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-2,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lr_scheduler = LambdaLR(optimizer, lambda _: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        wrapped_model, optimizer, train_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AModel(\n",
       "  (layer1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (layer2): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (C): CModel(\n",
       "    (C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, 0.05174887925386429\n",
      "Epoch 1, 0.07386156171560287\n",
      "Epoch 2, 0.018628951162099838\n",
      "Epoch 3, 0.03663861006498337\n",
      "Epoch 4, 0.022263865917921066\n",
      "Epoch 5, 0.062061410397291183\n",
      "Epoch 6, 0.040121085941791534\n",
      "Epoch 7, 0.019479941576719284\n",
      "Epoch 8, 0.03244991600513458\n",
      "Epoch 9, 0.057142555713653564\n",
      "Epoch 10, 0.015966355800628662\n",
      "Epoch 11, 0.03696296736598015\n",
      "Epoch 12, 0.08301135152578354\n",
      "Epoch 13, 0.0877738744020462\n",
      "Epoch 14, 0.029905393719673157\n",
      "Epoch 15, 0.007747014053165913\n",
      "Epoch 16, 0.1424693614244461\n",
      "Epoch 17, 0.015622764825820923\n",
      "Epoch 18, 0.037107523530721664\n",
      "Epoch 19, 0.05418482422828674\n",
      "Epoch 20, 0.01611332967877388\n",
      "Epoch 21, 0.017393141984939575\n",
      "Epoch 22, 0.014837323687970638\n",
      "Epoch 23, 0.028957057744264603\n",
      "Epoch 24, 0.01763251982629299\n",
      "Epoch 25, 0.016636045649647713\n",
      "Epoch 26, 0.008520846255123615\n",
      "Epoch 27, 0.0018330835737287998\n",
      "Epoch 28, 0.039698194712400436\n",
      "Epoch 29, 0.05326563119888306\n",
      "Epoch 30, 0.039649829268455505\n",
      "Epoch 31, 0.03293096646666527\n",
      "Epoch 32, 0.03263557329773903\n",
      "Epoch 33, 0.02196693792939186\n",
      "Epoch 34, 0.029183659702539444\n",
      "Epoch 35, 0.01313843671232462\n",
      "Epoch 36, 0.054565392434597015\n",
      "Epoch 37, 0.006621986627578735\n",
      "Epoch 38, 0.01379462145268917\n",
      "Epoch 39, 0.004909946583211422\n",
      "Epoch 40, 0.009398984722793102\n",
      "Epoch 41, 0.0328662246465683\n",
      "Epoch 42, 0.03792770951986313\n",
      "Epoch 43, 0.011741884052753448\n",
      "Epoch 44, 0.021038241684436798\n",
      "Epoch 45, 0.005901644006371498\n",
      "Epoch 46, 0.006194215267896652\n",
      "Epoch 47, 0.0038807946257293224\n",
      "Epoch 48, 0.059276849031448364\n",
      "Epoch 49, 0.017639437690377235\n",
      "Epoch 50, 0.026091527193784714\n",
      "Epoch 51, 0.05225005000829697\n",
      "Epoch 52, 0.010282033123075962\n",
      "Epoch 53, 0.008576459251344204\n",
      "Epoch 54, 0.04156516492366791\n",
      "Epoch 55, 0.011118154972791672\n",
      "Epoch 56, 0.05730782821774483\n",
      "Epoch 57, 0.012437371537089348\n",
      "Epoch 58, 0.006434870418161154\n",
      "Epoch 59, 0.019698038697242737\n",
      "Epoch 60, 0.031091634184122086\n",
      "Epoch 61, 0.04337682947516441\n",
      "Epoch 62, 0.0207267627120018\n",
      "Epoch 63, 0.015312382020056248\n",
      "Epoch 64, 0.0041576833464205265\n",
      "Epoch 65, 0.016646943986415863\n",
      "Epoch 66, 0.03049071505665779\n",
      "Epoch 67, 0.03427882492542267\n",
      "Epoch 68, 0.03473813831806183\n",
      "Epoch 69, 0.019107086583971977\n",
      "Epoch 70, 0.052817605435848236\n",
      "Epoch 71, 0.0069117373786866665\n",
      "Epoch 72, 0.026987243443727493\n",
      "Epoch 73, 0.022925499826669693\n",
      "Epoch 74, 0.02223091758787632\n",
      "Epoch 75, 0.07427133619785309\n",
      "Epoch 76, 0.05159829556941986\n",
      "Epoch 77, 0.009277568198740482\n",
      "Epoch 78, 0.02180851437151432\n",
      "Epoch 79, 0.0336855873465538\n",
      "Epoch 80, 0.0085427425801754\n",
      "Epoch 81, 0.018359839916229248\n",
      "Epoch 82, 0.03777562826871872\n",
      "Epoch 83, 0.018114101141691208\n",
      "Epoch 84, 0.021924935281276703\n",
      "Epoch 85, 0.016795925796031952\n",
      "Epoch 86, 0.11068864166736603\n",
      "Epoch 87, 0.0091909971088171\n",
      "Epoch 88, 0.06629650294780731\n",
      "Epoch 89, 0.04594694450497627\n",
      "Epoch 90, 0.013901978731155396\n",
      "Epoch 91, 0.043754275888204575\n",
      "Epoch 92, 0.057318925857543945\n",
      "Epoch 93, 0.03848601505160332\n",
      "Epoch 94, 0.013336200267076492\n",
      "Epoch 95, 0.07712908089160919\n",
      "Epoch 96, 0.07836391776800156\n",
      "Epoch 97, 0.014038982801139355\n",
      "Epoch 98, 0.0761447548866272\n",
      "Epoch 99, 0.030367303639650345\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        out = model_a(x)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(out, y, reduction=\"mean\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=False)\n",
    "\n",
    "    print(f\"Epoch {epoch}, {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.0649, -0.4435],\n",
       "                      [-0.0644,  0.4817],\n",
       "                      [-0.5576, -0.1599],\n",
       "                      [-0.6445,  0.1108]], device='cuda:0')),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.4020,  0.1738, -0.3677,  0.3300], device='cuda:0')),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.1744,  0.0382,  0.2159,  0.4484],\n",
       "                      [-0.4703,  0.1845,  0.3628, -0.0399],\n",
       "                      [ 0.3475,  0.1668, -0.2921,  0.4301],\n",
       "                      [-0.1527,  0.0454,  0.0541, -0.3027],\n",
       "                      [-0.2219, -0.2032,  0.2916,  0.2315],\n",
       "                      [-0.4922, -0.2139, -0.0118,  0.1270],\n",
       "                      [-0.1604,  0.1043, -0.2793,  0.2810],\n",
       "                      [ 0.3029, -0.2067, -0.3825,  0.1868]], device='cuda:0')),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.2963, -0.3305, -0.0682, -0.1248, -0.4390, -0.3795,  0.0197, -0.4016],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.weight',\n",
       "              tensor([[-2.2917,  0.1696, -0.2284,  1.7071, -0.9251,  0.1303,  1.9106, -1.5647],\n",
       "                      [ 0.9515, -0.4588,  0.1423, -1.3181,  0.5206, -0.3758, -1.9560,  2.1384],\n",
       "                      [-1.4055,  0.1905, -0.1881,  1.1676, -0.5467,  0.1760,  1.4395, -1.2554],\n",
       "                      [-0.8454,  0.4137, -0.1135,  1.0330, -0.4004,  0.2277,  1.4759, -1.5198]],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.bias',\n",
       "              tensor([ 0.1862, -0.0529,  0.0715,  0.0078], device='cuda:0')),\n",
       "             ('C.C_layer2.weight',\n",
       "              tensor([[ 1.8218, -0.4483,  0.5315,  0.2989],\n",
       "                      [-0.2816, -0.0704, -0.0431,  0.1065]], device='cuda:0')),\n",
       "             ('C.C_layer2.bias', tensor([0.9467, 0.5777], device='cuda:0'))])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.0649, -0.4435],\n",
       "                      [-0.0644,  0.4817],\n",
       "                      [-0.5576, -0.1599],\n",
       "                      [-0.6445,  0.1108]], device='cuda:0')),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.4020,  0.1738, -0.3677,  0.3300], device='cuda:0')),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.1744,  0.0382,  0.2159,  0.4484],\n",
       "                      [-0.4703,  0.1845,  0.3628, -0.0399],\n",
       "                      [ 0.3475,  0.1668, -0.2921,  0.4301],\n",
       "                      [-0.1527,  0.0454,  0.0541, -0.3027],\n",
       "                      [-0.2219, -0.2032,  0.2916,  0.2315],\n",
       "                      [-0.4922, -0.2139, -0.0118,  0.1270],\n",
       "                      [-0.1604,  0.1043, -0.2793,  0.2810],\n",
       "                      [ 0.3029, -0.2067, -0.3825,  0.1868]], device='cuda:0')),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.2963, -0.3305, -0.0682, -0.1248, -0.4390, -0.3795,  0.0197, -0.4016],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.weight',\n",
       "              tensor([[-2.1965,  0.1274, -0.1478,  1.6538, -0.8647,  0.0758,  1.7803, -1.6703],\n",
       "                      [ 0.6822, -0.3914,  0.0701, -1.1564,  0.3975, -0.3419, -1.7036,  2.1342],\n",
       "                      [-1.4889,  0.0377, -0.1577,  1.0793, -0.5144,  0.0743,  1.1861, -1.0327],\n",
       "                      [-0.9360,  0.2829, -0.0539,  0.9223, -0.3998,  0.0737,  1.1937, -1.2110]],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.bias',\n",
       "              tensor([ 0.2428, -0.0762,  0.1133,  0.0529], device='cuda:0')),\n",
       "             ('C.C_layer2.weight',\n",
       "              tensor([[ 1.7741, -0.5270,  0.6885,  0.4569],\n",
       "                      [-0.2450, -0.2079, -0.1109,  0.0317]], device='cuda:0')),\n",
       "             ('C.C_layer2.bias', tensor([0.7683, 0.6227], device='cuda:0'))])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrappedModel(\n",
       "  (C.C_layer1): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (C.C_layer2): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.unwrap_model(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('C.C_layer1.weight',\n",
       "              tensor([[ 4.1467,  0.4966,  1.7386, -0.3810,  0.2673, -0.9106,  1.8412, -0.0341],\n",
       "                      [ 2.8152,  0.4204,  1.2243, -0.5114,  0.4369, -0.6324,  1.8980, -0.0629],\n",
       "                      [-0.2590, -0.0796, -0.1273, -0.0409, -0.0426, -0.0321, -0.6010, -0.0057],\n",
       "                      [ 3.2758,  0.6608,  1.3855, -0.4597,  0.3026, -0.6473,  2.0962, -0.0068]],\n",
       "                     device='cuda:0')),\n",
       "             ('C.C_layer1.bias',\n",
       "              tensor([ 0.1723,  0.1349, -0.0129,  0.0912], device='cuda:0')),\n",
       "             ('C.C_layer2.weight',\n",
       "              tensor([[ 1.0318,  0.9485, -0.0579,  0.8989],\n",
       "                      [-0.1322, -0.0972,  0.0012, -0.1086]], device='cuda:0')),\n",
       "             ('C.C_layer2.bias', tensor([0.9125, 0.7507], device='cuda:0'))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader import load_diffusion_model\n",
    "\n",
    "kwargs = {\"is_lora\":True, \"lora_scale\":1.0}\n",
    "models = load_diffusion_model(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = models['diffusion'].named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_module = iter(module_list).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = {}\n",
    "\n",
    "def recursice_named_children(name, cur_module):\n",
    "    for subname, module in cur_module.named_children():\n",
    "        recursice_named_children(f\"{name}.{subname}\",module)\n",
    "        if \"lora\" in subname:\n",
    "            processor[f\"{name}.{subname}\"] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursice_named_children(cur_module[0], cur_module[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unet.encoders.1.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.1.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.encoders.2.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.4.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.encoders.5.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.7.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.encoders.8.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.bottleneck.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.3.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.4.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.k_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.v_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_1.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.k_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.q_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.q_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.v_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.out_lora_down': Linear(in_features=1280, out_features=4, bias=False),\n",
       " 'unet.decoders.5.1.attention_2.out_lora_up': Linear(in_features=4, out_features=1280, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.6.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.7.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.k_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.v_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_1.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.k_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.q_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.q_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.v_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.out_lora_down': Linear(in_features=640, out_features=4, bias=False),\n",
       " 'unet.decoders.8.1.attention_2.out_lora_up': Linear(in_features=4, out_features=640, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.9.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.10.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.k_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.v_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_1.out_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.k_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.k_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.q_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.q_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.v_lora_down': Linear(in_features=768, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.v_lora_up': Linear(in_features=4, out_features=320, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.out_lora_down': Linear(in_features=320, out_features=4, bias=False),\n",
       " 'unet.decoders.11.1.attention_2.out_lora_up': Linear(in_features=4, out_features=320, bias=False)}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "color_list = {}\n",
    "with open(\"/home/mlfavorfit/lib/favorfit/shlee/favorfit_color_recommendation/features/list_of_colors.jsonl\", mode=\"r\") as file:\n",
    "    for line in file:\n",
    "        line = json.loads(line)\n",
    "        color_list[line[\"color_number\"]] = line[\"color_rgb\"]\n",
    "\n",
    "color_list = [cur[1] for cur in sorted(color_list.items(), key=lambda x:x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_euclidien_similarity(data_arr):\n",
    "    data_arr = np.array(data_arr)\n",
    "    norm_data = np.sum(data_arr ** 2, axis=1).reshape(-1, 1)\n",
    "    squared_distances = norm_data + norm_data.T - 2 * np.dot(data_arr, data_arr.T)\n",
    "    squared_distances = np.maximum(squared_distances, 0)\n",
    "    distances = np.sqrt(squared_distances)\n",
    "    similarities = 1 / (1 + distances)\n",
    "    np.fill_diagonal(similarities, 1)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def get_color_tokens(palette):\n",
    "    palette = np.array(palette)\n",
    "    color_array = np.array(color_list)+1\n",
    "\n",
    "    rgb_results = []\n",
    "    id_results = []\n",
    "    for target in palette:\n",
    "        similarities = extract_euclidien_similarity(np.concatenate([target[None,:], color_array]))[0][1:]\n",
    "\n",
    "        id_results.append(np.argmax(similarities).tolist())\n",
    "        rgb_results.append((color_array[np.argmax(similarities)]-1).tolist())\n",
    "        \n",
    "    return rgb_results, id_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color_features = []\n",
    "with open(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_base/favorfit_product_color_train.jsonl\", mode=\"r\") as file:\n",
    "    for line in file:\n",
    "        line = json.loads(line)\n",
    "  \n",
    "        rgb_results, id_results = get_color_tokens(line[\"obj_colors\"][0])\n",
    "        img_color_features.append({\"image\":line[\"image\"], \n",
    "                                   \"obj_colors\":line[\"obj_colors\"][0], \n",
    "                                   \"cluster_colors\":rgb_results,\n",
    "                                   \"cluster_colors_id\":id_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp.json\", mode=\"w\") as f:\n",
    "    json.dump(img_color_features, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_rgb_colors(rgb_colors):\n",
    "    rgb_colors = np.array(rgb_colors)\n",
    "\n",
    "    # Create a figure and axis for the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Loop through the list of BGR colors and plot each one\n",
    "    for i, color in enumerate(rgb_colors):\n",
    "        # Convert BGR to RGB and normalize to [0, 1]\n",
    "        rgb_color = [x / 255.0 for x in color]\n",
    "\n",
    "        # Create a rectangle filled with the normalized RGB color\n",
    "        rect = plt.Rectangle((i, 0), 1, 1, facecolor=rgb_color)\n",
    "        \n",
    "        # Add the rectangle to the plot\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Set axis limits and aspect ratio\n",
    "    ax.set_xlim(0, len(rgb_colors))\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Remove axis labels and ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAACUCAYAAAD2x9FyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAADaUlEQVR4nO3asYozVRiA4TPJNlvkBkJyA7aCd6WlrRdg5fVZZgkigkWQbdwZqxXhJfww/GZ+3edpJ2f4IAznZeZMy7IsAwDgH3ZbDwAAfHkEAgAQAgEACIEAAIRAAABCIAAAIRAAgHhau3Ce53G9XsfhcBjTNH3OmQCAf8myLON2u43j8Th2u/vvCVYHwvV6Hefzee1yAGBDl8tlnE6nu9dXB8LhcBhjjPHV19+M/X71bfgP+fW337cegQf6/tvvth6BB/rhx5+2HoEHWea38ccvP/+9j9+zemd//6yw3z+N/ZNA+Ah2u/3WI/BAz8/PW4/AA02e7w/nU8cDHFIEAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAxNPahcuyjDHGeHv787MNw5dtnt+2HoEHen193XoEHmjxfH8Y7//1+z5+z7R86hd3vLy8jPP5vGYpALCxy+UyTqfT3eurA2Ge53G9XsfhcBjTNK0eEAB4nGVZxu12G8fjcex2908arA4EAOD/yyFFACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAADEX9yKVngOfOkbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAACUCAYAAAD2x9FyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAADa0lEQVR4nO3aT6rrZByA4a/tGajQDZR2D6JLcuYGHLsDJ66wh3BAwUFB7+Qkjo4IL/VCuDb3z/NMky/8IIG8JN9uWZZlAAD8y37rAQCAj49AAABCIAAAIRAAgBAIAEAIBAAgBAIAEE9rF87zPKZpGsfjcex2uw85EwDwP1mWZdxut3E6ncZ+f/87wepAmKZpXC6XtcsBgA1dr9dxPp/vHl8dCMfjcYwxxrfffT8Oh8Pay/AJ+eO337cegQf64ceftx6BB/r1l5+2HoEHmed5vLy8/PMev2d1ILz9VjgcDuPwtPoyfEIOeyH4Jfnq62+2HoEH+q9PzXye3rc9wBMBAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEACIEAAIRAAABCIAAAIRAAgBAIAEAIBAAgBAIAEAIBAAiBAACEQAAAQiAAACEQAIAQCABACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEAiKe1C5dlGWOM8fr6+sGG4eP2OrvXX5J3f/259Qg80DzPW4/Ag7zd67f3+D275X1n3PH8/Dwul8uapQDAxq7X6zifz3ePrw6EeZ7HNE3jeDyO3W63ekAA4HGWZRm3222cTqex39/fabA6EACAz5dNigBACAQAIAQCABACAQAIgQAAhEAAAEIgAAAhEACAEAgAQAgEACAEAgAQAgEAiL8B4GhWeAlXKiwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_rgb_colors(img_color_features[0][\"obj_colors\"])\n",
    "visualize_rgb_colors(img_color_features[0][\"cluster_colors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
