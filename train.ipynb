{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfavorfit/anaconda3/envs/diffusion/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "def make_train_dataset(path, tokenizer, accelerator):\n",
    "    dataset = load_dataset(path)\n",
    "    column_names = dataset['train'].column_names\n",
    "    image_column, conditioning_image_column, caption_column = column_names\n",
    "\n",
    "    image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        images = [image_transforms(image) for image in images]\n",
    "\n",
    "        conditioning_images = [image.convert(\"RGB\") for image in examples[conditioning_image_column]]\n",
    "        conditioning_images = [conditioning_image_transforms(image) for image in conditioning_images]\n",
    "        tokenized_ids = tokenizer.batch_encode_plus(examples[caption_column], padding=\"max_length\", max_length=77).input_ids\n",
    "       \n",
    "        examples[\"pixel_values\"] = images\n",
    "        examples[\"conditioning_pixel_values\"] = conditioning_images\n",
    "        examples[\"input_ids\"] = tokenized_ids\n",
    "\n",
    "        return examples\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "    \n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 00:39:57.527526: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-15 00:39:57.553006: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-15 00:39:57.681102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 00:39:58.164266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# accelerate 상에서 print문을 구현하기 위함임\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger 초기 configuration 지정하는 방법\n",
    "\n",
    "import os\n",
    "cur_dir = os.path.dirname(os.path.abspath(__name__))\n",
    "\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=os.path.join(cur_dir, \"training\"),\n",
    "    logging_dir=os.path.join(cur_dir, \"training\", \"log\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=1,\n",
    "    mixed_precision=\"fp16\",\n",
    "    # log_with=\"wandb\",\n",
    "    project_config=accelerator_project_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process:\n",
    "    os.makedirs(cur_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_converter import convert_model, convert_controlnet_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_state_dict = torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/v1-5-pruned-emaonly.ckpt\")[\"state_dict\"]\n",
    "control_state_dict = torch.load(\"/home/mlfavorfit/lib/favorfit/kjg/0_model_weights/diffusion/controlnet/control_v11f1e_sd15_tile.pth\")\n",
    "\n",
    "diffusion_state_dict = convert_model(diffusion_state_dict)\n",
    "control_state_dict = convert_controlnet_model(control_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader import load_diffusion_model, load_controlnet_model\n",
    "models = load_diffusion_model(diffusion_state_dict)\n",
    "# controlnet = load_controlnet_model(state_dict=control_state_dict, dtype=torch.float32)\n",
    "controlnet = load_controlnet_model(state_dict=None, dtype=torch.float32)\n",
    "\n",
    "models.update(controlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f68ecb7e230>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = torch.Generator(device=\"cuda\")\n",
    "generator.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpm import DDPMSampler\n",
    "\n",
    "sampler = DDPMSampler(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = models['clip']\n",
    "encoder = models['encoder']\n",
    "decoder = models['decoder'] \n",
    "diffusion = models['diffusion']\n",
    "controlnet = models['controlnet']\n",
    "embedding = models['controlnet_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ControlNetConditioningEmbedding(\n",
       "  (conv_in): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (blocks): ModuleList(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv_out): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.requires_grad_(False)\n",
    "encoder.requires_grad_(False)\n",
    "decoder.requires_grad_(False)\n",
    "diffusion.requires_grad_(False)\n",
    "\n",
    "controlnet.train()\n",
    "embedding.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_optimize = list(controlnet.parameters()) + list(embedding.parameters())\n",
    "optimizer = AdamW(\n",
    "        params_to_optimize,\n",
    "        lr=1e-5,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-08,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_train_dataset(\"/media/mlfavorfit/sdb/contolnet_dataset/fill50k\", tokenizer, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in examples])\n",
    "    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=3,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lambda _: 1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet, embedding, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    controlnet, embedding, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diffusion(\n",
       "  (time_embedding): TimeEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (unet): UNET(\n",
       "    (encoders): ModuleList(\n",
       "      (0): SwitchSequential(\n",
       "        (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1-2): 2 x SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Identity()\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=320, out_features=960, bias=False)\n",
       "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (conv_output): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): SwitchSequential(\n",
       "        (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Identity()\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): SwitchSequential(\n",
       "        (0): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (7): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Identity()\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): SwitchSequential(\n",
       "        (0): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (10-11): 2 x SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bottleneck): SwitchSequential(\n",
       "      (0): UNET_ResidualBlock(\n",
       "        (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (residual_layer): Identity()\n",
       "      )\n",
       "      (1): UNET_AttentionBlock(\n",
       "        (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention_1): SelfAttention(\n",
       "          (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention_2): CrossAttention(\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): UNET_ResidualBlock(\n",
       "        (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (residual_layer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (decoders): ModuleList(\n",
       "      (0-1): 2 x SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Upsample(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3-4): 2 x SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=320, out_features=960, bias=False)\n",
       "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (conv_output): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x SwitchSequential(\n",
       "        (0): UNET_ResidualBlock(\n",
       "          (groupnorm_feature): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv_feature): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (linear_time): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (groupnorm_merged): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv_merged): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (residual_layer): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): UNET_AttentionBlock(\n",
       "          (groupnorm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (conv_input): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (layernorm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_1): SelfAttention(\n",
       "            (in_proj): Linear(in_features=320, out_features=960, bias=False)\n",
       "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (layernorm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention_2): CrossAttention(\n",
       "            (q_proj): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=320, bias=False)\n",
       "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (layernorm_3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_geglu_1): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          (linear_geglu_2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (conv_output): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final): UNET_OutputLayer(\n",
       "    (groupnorm): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.to(accelerator.device, dtype=weight_dtype)\n",
    "encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "decoder.to(accelerator.device, dtype=weight_dtype)\n",
    "diffusion.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"validation_prompt\":[\"red circle with blue background\",  \"cyan circle with brown floral background\"], \n",
    "        \"validation_image\":[\"/media/mlfavorfit/sdb/contolnet_dataset/fill50k/validation/conditioning_image_1.png\", \"/media/mlfavorfit/sdb/contolnet_dataset/fill50k/validation/conditioning_image_2.png\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pipeline import generate_controlnet\n",
    "from PIL import Image\n",
    "def log_validation(encoder, decoder, clip, tokenizer, diffusion, controlnet, embedding, accelerator):\n",
    "    logger.info(\"Running validation... \")\n",
    "\n",
    "    controlnet = accelerator.unwrap_model(controlnet)\n",
    "    embedding = accelerator.unwrap_model(embedding)\n",
    "\n",
    "    models = {}\n",
    "    models['clip'] = clip\n",
    "    models['encoder'] = encoder\n",
    "    models['decoder'] = decoder\n",
    "    models['diffusion'] = diffusion\n",
    "    models['controlnet'] = controlnet\n",
    "    models['controlnet_embedding'] = embedding\n",
    "\n",
    "    image_logs = []\n",
    "    for validation_prompt, validation_image in zip(args[\"validation_prompt\"], args[\"validation_image\"]):\n",
    "        validation_image = Image.open(validation_image).convert(\"RGB\")\n",
    "\n",
    "        output_image = generate_controlnet(\n",
    "            prompt=validation_prompt,\n",
    "            uncond_prompt=\"\",\n",
    "            input_image=None,\n",
    "            control_image=validation_image,\n",
    "            do_cfg=True,\n",
    "            cfg_scale=7.5,\n",
    "            sampler_name=\"ddpm\",\n",
    "            n_inference_steps=20,\n",
    "            strength=1.0,\n",
    "            models=models,\n",
    "            seed=12345,\n",
    "            device=accelerator.device,\n",
    "            idle_device=\"cuda\",\n",
    "            tokenizer=tokenizer,\n",
    "            leave_tqdm=False\n",
    "        )\n",
    "\n",
    "        image = Image.fromarray(output_image)\n",
    "\n",
    "        image_logs.append(\n",
    "            {\"validation_image\": validation_image, \"images\": image, \"validation_prompt\": validation_prompt}\n",
    "        )\n",
    "\n",
    "    for tracker in accelerator.trackers:\n",
    "        if tracker.name == \"wandb\":\n",
    "            formatted_images = []\n",
    "\n",
    "            for log in image_logs:\n",
    "                image = log[\"images\"]\n",
    "                validation_prompt = log[\"validation_prompt\"]\n",
    "                validation_image = log[\"validation_image\"]\n",
    "\n",
    "                formatted_images.append(wandb.Image(validation_image, caption=\"Controlnet conditioning\"))\n",
    "                formatted_images.append(wandb.Image(image, caption=validation_prompt))\n",
    "\n",
    "            tracker.log({\"validation\": formatted_images})\n",
    "\n",
    "    return image_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process:\n",
    "    # tracker_config = dict(vars({args}))\n",
    "    tracker_config = args.copy()\n",
    "\n",
    "    tracker_config.pop(\"validation_prompt\")\n",
    "    tracker_config.pop(\"validation_image\")\n",
    "\n",
    "    accelerator.init_trackers(\"train_controlnet\", config=tracker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "first_epoch = 0\n",
    "initial_global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/166670 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_train_epochs = 10\n",
    "progress_bar = tqdm(\n",
    "    range(0, num_train_epochs * len(train_dataloader)),\n",
    "    initial=initial_global_step,\n",
    "    desc=\"Steps\",\n",
    "    # Only show the progress bar once on each machine.\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(timestep, dtype=torch.float16):\n",
    "    freqs = torch.pow(10000, -torch.arange(start=0, end=160, dtype=dtype) / 160) \n",
    "    x = torch.tensor(timestep, dtype=dtype)[:, None] * freqs[None]\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class ColorPaletteEmbedding(nn.Module):\n",
    "    def __init__(self, num_colors=4, n_embd=768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_colors=num_colors\n",
    "\n",
    "        self.cl_encoder  = nn.Sequential(\n",
    "            nn.Linear(3, n_embd//8),\n",
    "            nn.LayerNorm(n_embd//8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(n_embd//8, n_embd//4),\n",
    "            nn.LayerNorm(n_embd//4),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(n_embd//4, n_embd//2),\n",
    "            nn.LayerNorm(n_embd//2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(n_embd//2, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cl_encoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4384/2540512556.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(timestep, dtype=dtype)[:, None] * freqs[None]\n",
      "Steps:   0%|          | 6/166670 [00:04<31:55:43,  1.45it/s, loss=0.43, lr=1e-5]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m target \u001b[39m=\u001b[39m noise\n\u001b[1;32m     46\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(model_pred\u001b[39m.\u001b[39mfloat(), target\u001b[39m.\u001b[39mfloat(), reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m accelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m     49\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/accelerate/accelerator.py:1851\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1849\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1851\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "latents_shape = (1, 4, 64, 64)\n",
    "\n",
    "for epoch in range(first_epoch, num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        latents = encoder(batch[\"pixel_values\"].to(dtype=weight_dtype))\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        batch_size = batch['pixel_values'].shape[0]\n",
    "        \n",
    "        timesteps = torch.randint(0, sampler.num_train_timesteps, (batch_size,), device=\"cpu\").long()\n",
    "\n",
    "        latents = sampler.add_noise(latents, sampler.timesteps[timesteps], noise)\n",
    "        #---------------------------\n",
    "        input_temp = torch.randn([3,4,3])\n",
    "        colorpalette_model = ColorPaletteEmbedding(4, 768)\n",
    "        context_cat = colorpalette_model(input_temp).to(\"cuda\").to(dtype=weight_dtype)\n",
    "        #----------------------------\n",
    "        contexts = clip(batch['input_ids'])\n",
    "        contexts = torch.cat([contexts, context_cat], 1)\n",
    "        \n",
    "        control_image = batch[\"conditioning_pixel_values\"].to(latents.device)\n",
    "        control_latents = embedding(control_image).to(dtype=weight_dtype)\n",
    "\n",
    "        time_embeddings = get_time_embedding(timesteps).to(latents.device)\n",
    "\n",
    "        controlnet_downs, controlnet_mids = controlnet(\n",
    "            original_sample=latents, \n",
    "            latent=control_latents, \n",
    "            context=contexts,\n",
    "            time=time_embeddings\n",
    "        )\n",
    "        \n",
    "        model_pred = diffusion(\n",
    "            latents,\n",
    "            contexts,\n",
    "            time_embeddings,\n",
    "            additional_res_condition=[\n",
    "                [cur.to(dtype=weight_dtype) for cur in controlnet_downs], \n",
    "                [cur.to(dtype=weight_dtype) for cur in controlnet_mids]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        target = noise\n",
    "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=False)\n",
    "\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % 5000 == 0:\n",
    "                    save_path = os.path.join(\"./training\", f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                if global_step % 1000 == 0:\n",
    "                    log_validation(encoder, \n",
    "                                decoder, \n",
    "                                clip, \n",
    "                                tokenizer, \n",
    "                                diffusion, \n",
    "                                controlnet, \n",
    "                                embedding, \n",
    "                                accelerator)\n",
    "        \n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        controlnet = accelerator.unwrap_model(controlnet)\n",
    "        embedding = accelerator.unwrap_model(embedding)\n",
    "        torch.save(controlnet, f\"./training/controlnet_{epoch}.pth\")\n",
    "        torch.save(embedding, f\"./training/embedding_{epoch}.pth\")\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "temp = load_dataset(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'text', 'colors']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9333, -0.8784, -0.8314, -0.8471, -0.8118, -0.7961, -0.2353, -0.2784,\n",
       "        -0.3333, -0.5922, -0.5765, -0.5843])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.FloatTensor(temp[\"train\"][0]['colors']['total']).flatten()/ 255.0 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# B 모델 정의\n",
    "class B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(B, self).__init__()\n",
    "        self.b_ln = nn.Linear(3,5)\n",
    "        # B 모델의 레이어들을 정의\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.b_ln(x)\n",
    "        # B 모델의 forward pass 정의\n",
    "        # ...\n",
    "\n",
    "# A 모델 정의\n",
    "class A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A, self).__init__()\n",
    "        self.a_ln = nn.Linear(2,3)\n",
    "        # A 모델의 레이어들을 정의\n",
    "        # ...\n",
    "\n",
    "        # B 모델 정의\n",
    "        self.B_model = B()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a_ln(x)\n",
    "        b_outputs = self.B_model(x)\n",
    "        return b_outputs\n",
    "\n",
    "# # A 모델과 B 모델 생성\n",
    "# model_A = A()\n",
    "\n",
    "# # A 모델의 weight를 고정\n",
    "# for param in model_A.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # B 모델의 매개변수만을 사용하여 optimizer를 정의\n",
    "# optimizer = optim.Adam(model_A.B_model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # A 모델 훈련 (B 모델은 여기서 훈련되지 않음)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for inputs, targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model_A(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A(\n",
       "  (a_ln): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (B_model): B(\n",
       "    (b_ln): Linear(in_features=3, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a_ln.weight',\n",
       "              tensor([[-0.3877,  0.6321],\n",
       "                      [ 0.5478,  0.5093],\n",
       "                      [ 0.5246,  0.1936]])),\n",
       "             ('a_ln.bias', tensor([-0.4857,  0.5796, -0.6629])),\n",
       "             ('B_model.b_ln.weight',\n",
       "              tensor([[ 0.5010, -0.4070,  0.4238],\n",
       "                      [-0.4543,  0.5650, -0.2707],\n",
       "                      [-0.3158, -0.4582,  0.3809],\n",
       "                      [-0.0378, -0.0386, -0.0901],\n",
       "                      [-0.0066, -0.2748, -0.3092]])),\n",
       "             ('B_model.b_ln.bias',\n",
       "              tensor([-0.5354, -0.1291,  0.0920, -0.3835,  0.1404]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = model_A.B_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B(\n",
       "  (b_ln): Linear(in_features=3, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_ln.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0028,  0.2583,  0.4192],\n",
       "          [ 0.3938,  0.4438, -0.5401],\n",
       "          [-0.1793, -0.1946,  0.4920],\n",
       "          [-0.1165, -0.3382, -0.0060],\n",
       "          [-0.1397,  0.5581, -0.4283]], requires_grad=True)),\n",
       " ('b_ln.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.3106, -0.1701,  0.0876,  0.1845,  0.4864], requires_grad=True))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(cur, cur2) for cur, cur2 in model_B.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a_ln.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2269, -0.0564],\n",
       "          [ 0.2057,  0.6311],\n",
       "          [-0.1404,  0.1387]])),\n",
       " ('a_ln.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.6461,  0.6322, -0.6476])),\n",
       " ('B_model.b_ln.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0028,  0.2583,  0.4192],\n",
       "          [ 0.3938,  0.4438, -0.5401],\n",
       "          [-0.1793, -0.1946,  0.4920],\n",
       "          [-0.1165, -0.3382, -0.0060],\n",
       "          [-0.1397,  0.5581, -0.4283]], requires_grad=True)),\n",
       " ('B_model.b_ln.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.3106, -0.1701,  0.0876,  0.1845,  0.4864], requires_grad=True))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(cur, cur2) for cur, cur2 in model_A.named_parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
